\chapter{Introduction}
\label{sec:intro}
The machine learning \cite{65} aims to construct systems or applications that learn and improve with experience. Over the years, many machine learning systems have been developed which range from detecting fraud-detection \cite{56} for banking transactions, to facial recognition that can be used to pay at the supermarkets, to autonomous driving systems. In the medical field, diseases can be detected at a much early stage nowadays by training diagnostic models using patient's data in a hospital. The e-commerce players use a recommender system \cite{55} to suggest new products to the user based on their previous or historical data. However, machine learning is highly dependent upon private and sensitive user data. With all this data around for a long time, many attackers or adversaries try to steal this data, and hence this level of sensitive data needs protection from all such attacks in an un-trusted environment, for example, a public cloud. EU's General Data Protection Regulation (GDPR) \cite{57} is implementing stricter norms to define how user data should be handled and processed. As a consequence, the confidentiality and integrity of this data have become of paramount interest in an untrusted environment. Service providers need to adhere to both accountability and legal consequences for data mishandling, mismanagement, and leakage. Data must be protected not only during transmission and computation, but also when it is at rest.


In this thesis, we aim to build a machine learning framework that not only provides confidentiality and integrity to the data, code, model, and computation, but also provide privacy to the application. Confidentiality here means that the data, code, or model used for computation should be revealed only to the intended audience. Integrity means any kind of modification to any of these should be possible to detect. Confidentiality is usually addressed with the help of encryption/decryption. Encryption transforms the plain text to conceal it while decryption transforms the encrypted text into plain text. These techniques are achieved with the help of a single key or public-private key-pair. The integrity part is handled using message authentication which is the signing of data with cryptographic hash/signature \cite{58}. The details about confidentiality and integrity techniques are out of the scope of this thesis however can be studied in detail at \cite{58}.

To ensure the confidentiality and integrity of the cloud-native applications Trusted Execution Environments (TEEs) \cite{59} has emerged as one of the most promising technology. TEEs are hardware-assisted solutions that securely isolate the memory area through enclaves. Within these enclaves the data, code, model are executed safely and securely. Even if an adversary or attacker can compromise system software with root privileges, it is nearly impossible for an attacker to access the enclave's content (where the sensitive and private data/code is kept). There are many TEEs available in the market such as AMD Secure Processor \cite{19}, ARM Trust zone \cite{20}, Intel SGX \cite{9}, IBM secure execution \cite{21} etc. 

However, TEEs SDKs are complex, there are a lot of code changes required in the existing system for using TEEs SDK. To overcome this problem, we will be using the SCONE \cite{22} which provides a platform to run any existing application over TEEs without any code changes. Together with this combination, confidentiality and integrity can be added to any existing native application. For this thesis implementation, we choose Intel SGX as our TEEs because SCONE is currently developed using Intel SGX and both of these are well tested for performance and compatibility together.

TEEs look very promising to provide confidentiality and integrity guarantees however TEEs still cannot ensure privacy because an adversary can still hit the enclaves with the malicious query to the data which is inside the enclave and private information can be leaked by such queries. The privacy of the data means no private information about an individual should be leaked from this data. The adversary can query the dataset to gain some group information and with some prior knowledge about the subject, it can infer information with the help of multiple summary statistics queries such as count, sum, or average \cite{72}. Applying such attacks to the machine learning system can further lead to leakage of data, model, or model parameters. All such attacks on the machine learning system are discussed in detail in section ~\ref{sec:attackOnML}.


There are many techniques to achieve privacy goals such as approximation of activation functions with low degree polynomials \cite{60}, CryptoNets \cite{61},  SecureML \cite{62}. In our system SPML, we will be using a state-of-the-art privacy-preserving technique known as differential privacy proposed by Cynthia Dwork from Microsoft Research\cite{3}. Differential privacy \cite{3} states that no information about an individual should be learned from the dataset, while useful information about a population can be learned. For example, it can be learned from a medical database that smoking causes cancer, but it shouldn't be learned who all smoke. Hence, this notion about individual privacy can be protected using differential privacy. 

Recently there is a paper by Google \cite{4} to implement differential privacy, however, confidentiality and integrity guarantee is not ensured by this. We marriage privacy with confidentiality and integrity together in this thesis. To implement our system, we will be using TensorFlow \cite{46} framework by Google, Intel SGX \cite{9} as TEEs, and privacy paper by Google \cite{4}. We also explored a randomized response mechanism which is our contribution and work. In the evaluation chapter, it is shown that our system SPML provides confidentiality, integrity, and privacy guarantees however SPML makes training 25x and inference 6x slower as compared to the native system. We can say this is the cost of enabling security and privacy property for any existing machine learning system. The main advantage of using SPML is that with minimum code changes we can build any existing native machine learning or a deep learning system, a secure privacy-preserving system. SPML can be used with any cloud-native application and we can benefit from any cloud environment features like auto-scaling. The efforts required to change any existing machine learning application are very less and hence can save a lot of time required to add privacy, confidentiality, and integrity guarantee to the native machine learning system.

The thesis is organized in the form of different chapters. In the next chapter, some already existing work on these similar lines is discussed. After that background information about various technologies used in our system SPML is discussed in chapter 3. Chapter 4 talks about the design goals and detailed design of our system. Chapter 5, we have discussed the implementation details of our system. Chapter 6, evaluates our system for its performance and accuracy. Chapter 7 we have highlighted enhancement and future work required for making our system more usable. At the end of Chapter 8, we have concluded our work.
