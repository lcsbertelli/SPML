\chapter{Related Work}
\label{sec:related_work}
Our system SPML consists of a machine learning module, a privacy module, and a security component. The machine learning aims to provide the learning ability to the systems by training a model on the input data. These trained models can be used to predict or classify real-world data. The input data used for training these models contain sensitive and private information about an individual. Hence, confidentiality, and the integrity of an application, and the privacy of the individual whose data is involved in the dataset must be protected. We will be using differential privacy techniques \cite{3} in this thesis to address privacy concerns and Trusted Execution Environment to protect the confidentiality and integrity of an application. Differential privacy technique works by adding random noise on the input dataset. There are already many techniques proposed in the past about how to apply differential privacy on machine learning algorithms. We will discuss a few of these techniques first using a survey paper on differential privacy and machine learning and then later some start-of-the-art techniques as proposed by Google.


\section{Differential privacy and machine learning: a survey and review}
\label{sec:rwDPsurvery}
In this survey paper \cite{6}, a basic notion is discussed how differential privacy and machine learning can go hand in hand. At a high level, combining machine learning with privacy always requires a trade-off between privacy and accuracy level. This is because if too much noise is added to the input data, then data will become unusable as accuracy will reduce drastically. Hence, it requires some calibrated balance between accuracy and privacy. For example, if medical research is considered, it demands access to patient's records and at the same time, their privacy must be preserved. The challenge here is how to share generic information of patients without compromising their privacy. This problem is solved using differential privacy techniques \cite{3} which were proposed by Microsoft researcher Cynthia Dwork.

One could argue that if names and personal IDs are removed from the dataset/database, then data will be anonymous, as an adversary cannot learn anything about a person. However, if the adversary has some other related knowledge also known as auxiliary information about the user then such details can be used to conclude information that an adversary is looking for. For example, if a healthcare database from a hospital and voter registration information are combined, one may identify an intended person from the records by combining these two databases. Differential privacy introduces random noise in the publicly available data so that changes cannot be detected even if one row in the database is changed. Since the output is not dependent much on inputs, the adversary cannot get sensitive or any kind of information for any individual.

There are several groups of differentially private machine learning algorithms based on the approaches they use to preserve privacy. The first category is learning a model based on the raw data, then later applying the Laplace mechanism as proposed in \cite{26} \cite{27} \cite{28} \cite{29} \cite{30} or exponential mechanism using one of the techniques discussed in \cite{31} \cite{32} to generate privacy-preserving algorithms. In the second approach instead of first training the model on raw data, Laplace and exponential mechanisms are enforced on the output parameters after every iteration or step such as discussed in \cite{33} \cite{34} \cite{35} \cite{36} \cite{37} \cite{38} \cite{39}. The third category also known as objective perturbation is discussed in \cite{40} \cite{41} \cite{42} \cite{43}. In this type, the target function is made noisy by adding some noise to it and a minimum or maximum of this noisy function is used as an output model. The fourth category is based on the framework of sampling and aggregation as stated in \cite{44} \cite{45}. They work by splitting the input into several subsets, then the model is estimated by combining the results of all these subsets, and noise is added in the step of aggregation. 

However, all these papers and mechanisms don't define a general framework that can be used to train any kind of model. For example, if we are interested in applying the first approach, then it can be applied to train models using a few of the machine learning algorithms. The paper discussed in \cite{26} can only train models using naive Bayes classification. We cannot apply the same technique to train a model using a decision tree or logistic regression. Secondly, most of the approaches require us to either preprocess inputs or change the training algorithms which means adding a further overhead of changing existing code, which requires more effort. Most of this work is based on a convex model that has fewer parameters as compared to complex networks. Training complex networks that contain a large number of parameters such as deep learning models results in large privacy loss using these approaches as stated in \cite{4}. Hence, we need a framework that is generalized and can be used for any already deployed machine learning and deep learning systems with minimum code changes and efforts. Google has been actively contributing to this area and came up with paper for RAPPOR \cite{5} in 2014 which tries to explain how a randomized response mechanism can be applied in general to achieve differential privacy. We studied this paper to see if this paper addresses some of these shortcomings.

\section{RAPPOR-Randomized Aggregatable Privacy-Preserving Ordinal Response}
\label{sec:rwRappor}
RAPPOR paper \cite{5} talks about the start of the art method of achieving differential privacy using a randomized response. Most of the chrome users face an issue wherein an attacker wants to change the default home page on the browser but to find more about this type of attack, one needs to know what kind of homepages are used by the users which means invading the privacy of the user. The approach is to find an algorithm that aims to find a popular percentage of homepages or search engines like yahoo, bing, google, etc that are being used. RAPPOR finds the distribution of search engines without compromising the security of a particular user. It implements privacy to learn user statistics by imposing privacy guarantees for each user, and user data is not maintained in a database, hence there is no third party involved to put the trust into.

RAPPOR is built using the idea of randomized response \cite{14} proposed by Warner in the 1960s to collect survey results from people, wherein the confidentiality of the people participating in this survey is retained. This means after collecting data from many people, information can be learned about the population but nothing can be learned about the individuals participating in the survey. The approach here is that the URLs are converted to bloom filters and a subset is chosen from a big bit vector, and noise is added there. RAPPOR is executed on the client locally and the report is sent to the server. 


The algorithm is based on the client's value v and some parameters such as f, h, k, p, and q. First, the client value v is hashed into the bloom filter say B having size k with the help of h hash functions. Then a permanent randomized response B' is created for each value v and bits in the B with the help of probability f. The parameter f is a tunable parameter to control the longitudinal privacy guarantee level. B' is memoized and reused in all subsequent reports for this value v. The next step is to generate instantaneous randomized response S. S is obtained by modifying B' with the help of probabilities p and q. Then S is sent to the server. In the paper, it is also shown how differential privacy levels can be controlled with the help of these parameters.


 This paper shows the state-of-the-art way to use a randomized response mechanism to achieve differential privacy guarantees. It works well for heavy-hitter collection, but what about the scenarios in which the client answer changes with time even for the same query as point out in \cite{18}. The RAPPOR target a particular type of problem however, applying this in all variety of machine learning problems such as image or object classification, which are based on deep learning techniques, still require some more effort and work.
 
 Secondly, a user can log in through different devices like phone, PC, tablet, etc which may lead to another problem of information leakage if all these data is collected for a single event. Also, the number of the cohort (subset chosen from big vector) must be changed as it may help to track the clients and can reduce privacy. RAPPOR protects the privacy of the training dataset but what about protecting the model’s parameters. An adversary can use the trained model to attack the machine learning system using model inversion attack \cite{17} or membership inference attack \cite{16}. Hence, we need some other mechanism that protects the training dataset and the trained model’s parameters.

Google came up with paper for Deep Learning with differential privacy \cite{4} in 2016 which tries to generalize the application of differential privacy in any kind of machine learning framework including deep learning. It not only protects the training dataset but the model’s parameters also.
 
\section{Deep Learning with differential privacy}
\label{sec:rwDL}
In this paper \cite{4}, advanced privacy-preserving mechanisms are combined with state-of-the-art machine learning methods to train neural networks. Broadly in machine learning, there is a feedback loop wherein customers are motivated by the high utility of an application or platform and hence they are ready to share more and more private and sensitive data with the devices and computing platforms. All these devices and platforms become more powerful with the data shared with them. As per the standard pipeline for machine learning, training data is fed to the trained model and the inference engine uses these models to make inferences. 

Authors argued that machine learning systems already have techniques like regularization which can protect the training data. On the other hand, deep learning networks have complex internal representations and these representations might leak some of the information about training data as demonstrated in model inversion attack \cite{17} or membership inference attack \cite{16}. Hence authors have implemented and evaluated the algorithm with deep learning methods. 

This algorithm will not only protect the training dataset but also the model’s parameters. This algorithm has been implemented in TensorFlow \cite{46} released by Google in 2015 and aims to build a differentially private stochastic gradient descent algorithm (SGD). In the algorithm, the loss function is minimized at each iteration. It consists of computing gradient at each step for a random sample from the dataset, clipping the l2 norm of these gradients, computing the average, and then adding random noise to preserve privacy. 

To compute the privacy loss of this differentially private SGD, a privacy accountant \cite{49} procedure based on the composability property of differential privacy is used. Privacy cost is calculated on the training data on each access and is accumulated with the progression of training. Gradients are calculated for each iteration of training and this accountant is responsible for accumulating the cost for all of these iterations.

Hence, this work gives us the generalized framework to apply differential privacy not only on machine learning techniques but also on deep learning network techniques. Therefore, using this algorithm privacy of the data can be preserved even in complex networks. 

However, this paper doesn't talk about how to preserve the confidentiality and integrity of the data, model, code, and inference results in an untrusted environment. We can use this work for solving the privacy problem but security concerns cannot be addressed using the techniques discussed in this paper. This is where Trusted Execution Environments (TEEs) comes to the rescue, to provide the confidentiality and integrity to the applications.

\section{Trusted Execution Environment}
A trusted execution environment (TEE) \cite{59} aims to provide confidentiality and integrity to the data and code. It achieves this goal by executing code and data isolated from the other processes. An isolated memory region called enclave is defined where the code and data are executed. This isolation happens by utilizing hardware as well as software approaches. The chip contains private keys generated by the manufacturer, which protects the TEE content by any untrusted parts (i.e OS or hypervisor process or even root user) of the platform. 

One may argue that there are many existing software techniques for preserving the confidentiality and integrity of the data and code such as homomorphic encryption. With homomorphic encryption, encrypted data and model can be processed without decrypting them during training for machine learning or deep learning task, however, it incurs a high computation cost \cite{50} \cite{51}. On the other hand, TEEs don't incur such a high computation cost and its performance can be compared with the native runs of the untrusted environment such as public cloud for deep learning network \cite{52} \cite{53}. Hence, TEEs are a better choice in our case instead of using any other software approach/es.

The only problem with TEEs is that we need to change the native code of the application and add hardware instructions in the application to utilize benefits on TEEs. This requires an understanding of an additional set of hardware instructions and code modification which is an overhead for the developers. We overcome this overhead by using SCONE (Secure CONtainer Environment) \cite{22}. The SCONE is a confidential computing platform that facilitates execution in always encrypted form. The additional overhead of input/output encryption, application execution in isolated (encrypted) memory is simplified by SCONE. The SCONE is discussed in detail in the next section.

%\cleardoublepage
